{"cells":[{"cell_type":"markdown","metadata":{"id":"hOEUCmZtRH5i"},"source":["# Drive"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2325,"status":"ok","timestamp":1711889428552,"user":{"displayName":"Myller Silva","userId":"09755207846604823006"},"user_tz":180},"id":"827raG70Q03f","outputId":"be29bec0-1c9c-4e77-acef-dfc80f22ab0b"},"outputs":[],"source":["# # @title Montar o Google Drive\n","# from google.colab import drive\n","# drive.mount('/content/drive')\n","# base_path = '/content/drive/My Drive/explications-anns'\n","base_path = './'"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":28,"status":"ok","timestamp":1711889428553,"user":{"displayName":"Myller Silva","userId":"09755207846604823006"},"user_tz":180},"id":"fbH5qWFnRVkE"},"outputs":[],"source":["import os\n","def create_results_directory(dir_path:str, model_h5_file:str)->str:\n","  file_name_model = (f\"{dir_path}/{model_h5_file}\")\n","  file_name_model_result = (f\"{dir_path}/results/{model_h5_file}\")\n","  if not os.path.exists(file_name_model_result):\n","      os.makedirs(file_name_model_result)\n","  return file_name_model_result"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1711889428553,"user":{"displayName":"Myller Silva","userId":"09755207846604823006"},"user_tz":180},"id":"KlkS0DXhSBCT"},"outputs":[],"source":["datasets_path = f'{base_path}/datasets'"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def create_directory(path_to_create:str):\n","  array_splited = path_to_create.split('/')\n","  min_max_path_full = './'\n","  for path in array_splited:\n","    if path == '':\n","      continue\n","    min_max_path_full = f'{min_max_path_full}/{path}'\n","    if not os.path.exists(min_max_path_full):\n","      os.makedirs(min_max_path_full)\n","  return min_max_path_full"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["import tensorflow as tf"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1711889428554,"user":{"displayName":"Myller Silva","userId":"09755207846604823006"},"user_tz":180},"id":"tREthsSRSYii"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"]}],"source":["from dataclasses import dataclass\n","from typing import List\n","@dataclass\n","class Dataset:\n","    dir_path: str\n","    model: str\n","    n_classes: int\n","\n","dataset_name = 'iris'\n","n_layers = 5\n","dataset_config = Dataset(\n","  dir_path=f\"{datasets_path}/{dataset_name}\",\n","  model=f\"models/model_{n_layers}layers_20neurons.h5\",\n","  n_classes=10,\n",")\n","matrix_size = (2, 2) # 4 features\n","target_name = 'species'\n","\n","import pandas as pd\n","\n","create_directory(f'{dataset_config.dir_path}/results/{dataset_config.model}')\n","\n","results_df_file = f'{dataset_config.dir_path}/results/{dataset_config.model}/df.csv'\n","data_df_file = f'{dataset_config.dir_path}/{dataset_config.model}/df.csv'\n","results_df = pd.read_csv(results_df_file)\n","\n","\n","\n","def read_dataset(dir_path:str, model_h5_file:str):\n","  data_test = pd.read_csv(f\"{dir_path}/test.csv\")\n","  data_train = pd.read_csv(f\"{dir_path}/train.csv\")\n","  data = data_train._append(data_test)\n","\n","  model_h5 = tf.keras.models.load_model(f\"{dir_path}/{model_h5_file}\")\n","  return (data, model_h5)\n","\n","data, model_h5 = read_dataset(dir_path = f'{datasets_path}/{dataset_name}', model_h5_file = dataset_config.model)\n"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1711889428555,"user":{"displayName":"Myller Silva","userId":"09755207846604823006"},"user_tz":180},"id":"sEb9mNsKVLlu"},"outputs":[],"source":["def not_in_explanation(explanation: list, matriz_size: tuple[int, int]):\n","  x, y = matriz_size\n","  array = []\n","  for i in range(x):\n","    for j in range(y):\n","      if (i, j) not in explanation:\n","        array.append((i, j))\n","  return array"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1711889428991,"user":{"displayName":"Myller Silva","userId":"09755207846604823006"},"user_tz":180},"id":"RkR9QLuwd5Jd"},"outputs":[],"source":["results_df = results_df.query('(len_relaxado_global<4 or len_relaxado<4)')"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>instance_index</th>\n","      <th>tempo_original</th>\n","      <th>tempo_relaxado</th>\n","      <th>tempo_relaxado_global</th>\n","      <th>len_original</th>\n","      <th>len_relaxado</th>\n","      <th>len_relaxado_global</th>\n","      <th>delta</th>\n","      <th>explanation</th>\n","      <th>explanation_relaxed</th>\n","      <th>explanation_relaxed_global</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>16</th>\n","      <td>16</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.080396</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>3</td>\n","      <td>0.1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>[(0, 1), (1, 0), (1, 1)]</td>\n","    </tr>\n","    <tr>\n","      <th>60</th>\n","      <td>60</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.740139</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>3</td>\n","      <td>0.1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>[(0, 1), (1, 0), (1, 1)]</td>\n","    </tr>\n","    <tr>\n","      <th>61</th>\n","      <td>61</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.745890</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>3</td>\n","      <td>0.1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>[(0, 1), (1, 0), (1, 1)]</td>\n","    </tr>\n","    <tr>\n","      <th>95</th>\n","      <td>95</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.836696</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>3</td>\n","      <td>0.1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>[(0, 1), (1, 0), (1, 1)]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    instance_index  tempo_original  tempo_relaxado  tempo_relaxado_global  \\\n","16              16             NaN             NaN               0.080396   \n","60              60             NaN             NaN               0.740139   \n","61              61             NaN             NaN               0.745890   \n","95              95             NaN             NaN               0.836696   \n","\n","    len_original  len_relaxado  len_relaxado_global  delta  explanation  \\\n","16           NaN           NaN                    3    0.1          NaN   \n","60           NaN           NaN                    3    0.1          NaN   \n","61           NaN           NaN                    3    0.1          NaN   \n","95           NaN           NaN                    3    0.1          NaN   \n","\n","    explanation_relaxed explanation_relaxed_global  \n","16                  NaN   [(0, 1), (1, 0), (1, 1)]  \n","60                  NaN   [(0, 1), (1, 0), (1, 1)]  \n","61                  NaN   [(0, 1), (1, 0), (1, 1)]  \n","95                  NaN   [(0, 1), (1, 0), (1, 1)]  "]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["results_df"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>instance_index</th>\n","      <th>tempo_original</th>\n","      <th>tempo_relaxado</th>\n","      <th>tempo_relaxado_global</th>\n","      <th>len_original</th>\n","      <th>len_relaxado</th>\n","      <th>len_relaxado_global</th>\n","      <th>delta</th>\n","      <th>explanation</th>\n","      <th>explanation_relaxed</th>\n","      <th>explanation_relaxed_global</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["Empty DataFrame\n","Columns: [instance_index, tempo_original, tempo_relaxado, tempo_relaxado_global, len_original, len_relaxado, len_relaxado_global, delta, explanation, explanation_relaxed, explanation_relaxed_global]\n","Index: []"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["results_df.query(\"len_relaxado_global<len_relaxado\")"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1711889428991,"user":{"displayName":"Myller Silva","userId":"09755207846604823006"},"user_tz":180},"id":"9DciNLZde3UX"},"outputs":[],"source":["# colunas_pixel = data.filter(regex='^pixel').columns.tolist()\n","# df_pixel = data[colunas_pixel]"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":269,"status":"ok","timestamp":1711893158201,"user":{"displayName":"Myller Silva","userId":"09755207846604823006"},"user_tz":180},"id":"9ofFhqGXxAks"},"outputs":[],"source":["def coordenadas_para_indice(p:tuple[int, int], matriz_size:tuple[int, int]):\n","  linha, coluna = p\n","  num_linhas, num_colunas = matriz_size\n","  indice = linha * num_colunas + coluna\n","  return indice\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# results_df"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# qtd_testes = 10\n","# errors = []\n","# for index, row in results_df.iterrows():\n","#     print(f\"\\t{index}\")\n","    \n","#     instance_explanation = row \n","#     instance_explanation_index = instance_explanation['instance_index']\n","#     # print(data)\n","#     target = data.iloc[int(instance_explanation_index)][target_name]\n","    \n","#     explanation = eval(instance_explanation['explanation_relaxed_global'])\n","#     out_explanation = not_in_explanation(explanation, matriz_size)\n","#     out_explanation_indixes = [ coordenadas_para_indice(coordenada, matriz_size) for coordenada in out_explanation ] \n","    \n","#     instacia = data.iloc[instance_explanation_index][:-1].to_numpy().copy()\n","#     # print(instacia)\n","#     lb, ub = 0, 1\n","#     n = len(out_explanation)\n","    \n","#     predict_old_array = model_h5.predict(instacia.reshape(1, -1), verbose=0 )\n","#     for teste_i in range(qtd_testes):\n","#         valores_aleatorios = lb+(ub-lb)*np.random.random(n)\n","        \n","#         for i, x in enumerate(valores_aleatorios):\n","#             instacia[out_explanation_indixes[i]] = x \n","#         # print(instacia)\n","#         predict_random_array = model_h5.predict(instacia.reshape(1, -1), verbose=0 )\n","        \n","#         predict_old = predict_old_array.argmax()\n","#         predict_random_error = predict_random_array.argmax()\n","        \n","#         if predict_old != predict_random_error: \n","#             print(predict_old, predict_random_error)\n","#             error = {\n","#                 \"target\": target,\n","#                 \"teste_iteration\": teste_i,\n","#                 \"instance\": instance_explanation_index,\n","#                 \"predict_old\": predict_old_array.argmax(),\n","#                 \"predict_random_error\": predict_random_array.argmax(),\n","#                 \"predict_old_array\": predict_old_array,\n","#                 \"predict_random_error_array\": predict_random_array,\n","#             }\n","#             errors.append(error) \n","#             break\n","        \n","        "]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["def coor_to_index(ponto, matriz_size):\n","        x_lin, y_col = matriz_size\n","        x, y = ponto\n","        return x*y_col + y"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\t16\n"]},{"name":"stdout","output_type":"stream","text":["\t60\n","\t61\n","\t95\n"]}],"source":["qtd_testes = 10\n","errors = []\n","for index, row in results_df.iterrows():\n","    print(f\"\\t{index}\")\n","    \n","    instance_explanation = row \n","    instance_explanation_index = instance_explanation['instance_index']\n","    \n","    target = data.iloc[int(instance_explanation_index)][target_name]\n","    explanation = eval(instance_explanation['explanation_relaxed_global'])\n","    instacia = data.iloc[instance_explanation_index][:-1].to_numpy().copy()\n","    \n","    out_explanation = not_in_explanation(explanation, matrix_size)\n","    out_explanation_indixes = [ coordenadas_para_indice(coordenada, matrix_size) for coordenada in out_explanation ] \n","    \n","    lb, ub = 0, 1\n","    n = len(out_explanation_indixes)\n","    \n","    predict_old_array = model_h5.predict(instacia.reshape(1, -1), verbose=0 )\n","    for teste_i in range(qtd_testes):\n","        valores_aleatorios = lb+(ub-lb)*np.random.random(n)\n","        \n","        for i, x in enumerate(valores_aleatorios):\n","            instacia[out_explanation_indixes[i]] = x \n","            \n","        predict_random_array = model_h5.predict(instacia.reshape(1, -1), verbose=0 )\n","        predict_old = predict_old_array.argmax()\n","        predict_random_error = predict_random_array.argmax()\n","        \n","        \n","        \n","        if predict_old != predict_random_error: \n","            print(predict_old, predict_random_error)\n","            error = {\n","                \"target\": target,\n","                \"teste_iteration\": teste_i,\n","                \"instance\": instance_explanation_index,\n","                \"predict_old\": predict_old_array.argmax(),\n","                \"predict_random_error\": predict_random_array.argmax(),\n","                \"predict_old_array\": predict_old_array,\n","                \"predict_random_error_array\": predict_random_array,\n","            }\n","            errors.append(error) \n","            break\n","        \n","        "]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["taxa de erros: 0.00%\n"]}],"source":["print(f'taxa de erros: {(len(errors) / len(results_df) * 100):.2f}%')"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["%pip install tabulate\n","from tabulate import tabulate\n","from IPython.display import clear_output\n","clear_output()"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["target    instance    teste_iteration    predict_old    predict_random_error\n","--------  ----------  -----------------  -------------  ----------------------\n"]}],"source":["table_data = [] # transform to dataframe\n","for error in errors:\n","  table_data.append([error[\"target\"], error[\"instance\"], error[\"teste_iteration\"], error[\"predict_old\"], error[\"predict_random_error\"]])\n","\n","table_headers = [\"target\", \"instance\", \"teste_iteration\", \"predict_old\", \"predict_random_error\"]\n","\n","print(tabulate(table_data, headers=table_headers))"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"data":{"text/plain":["array([16, 60, 61, 95], dtype=int64)"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["results_df['instance_index'].index.to_numpy()"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"data":{"text/plain":["0"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["len(results_df.query('len_original > len_relaxado_global'))"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNmcnNizaV3cwJRmRviPIDh","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
