{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import os\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.api.models import load_model\n",
    "# from keras.api.models import Model as KerasModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.utils import (\n",
    "    generate_layers_to_relax,\n",
    "    linear_constraints_to_dict,\n",
    "    store_data,\n",
    ")\n",
    "\n",
    "from utils.milp import codify_network, relax_model\n",
    "from utils.explanations import get_explanation_range, get_minimal_explanation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(\n",
    "    results_path: str, \n",
    "    layers_idx: Optional[list[int]] = None\n",
    "):\n",
    "    # Datasets e modelos\n",
    "    datasets = [\n",
    "        # {\"nome\": \"iris\", \"n_classes\": 3},\n",
    "        # {\"nome\": \"wine\", \"n_classes\": 3},\n",
    "        # {\"nome\": \"breast_cancer\", \"n_classes\": 2},\n",
    "        # {\"nome\": \"glass\", \"n_classes\": 5},\n",
    "        {\"nome\": \"digits\", \"n_classes\": 10},\n",
    "        # {\"nome\":\"mnist\", \"n_classes\":10}, # rodar individualmente\n",
    "    ]\n",
    "    modelos = [\n",
    "        # \"model_1layers_20neurons.h5\",\n",
    "        # \"model_2layers_20neurons.h5\",\n",
    "        \"model_3layers_20neurons.h5\",\n",
    "    ]\n",
    "\n",
    "    # definir array de deltas a seres utilizados\n",
    "    # deltas = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1]\n",
    "    deltas = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "    # para cada dataset\n",
    "    for dataset in datasets:\n",
    "        data = pd.read_csv(f\"datasets/{dataset['nome']}/data.csv\")\n",
    "        # para cada modelo do dataset\n",
    "        for modelo in modelos:\n",
    "\n",
    "            # carregar modelo em formato h5\n",
    "            keras_model = load_model(\n",
    "                f\"datasets/{dataset['nome']}/models/{modelo}\", compile=False\n",
    "            )\n",
    "\n",
    "            # 1 - gerar modelos MILP\n",
    "            method = \"fischetti\"\n",
    "            model_milp, output_bounds = codify_network(keras_model, data, method, True)\n",
    "\n",
    "            # 2 - gerar modelo MILP relaxado\n",
    "            model_milp_relaxed = relax_model(\n",
    "                model=model_milp,\n",
    "                neural_network=keras_model, # type: ignore\n",
    "                layers_to_relax=generate_layers_to_relax(\n",
    "                    neural_network=keras_model, # type: ignore\n",
    "                    layers_idx=layers_idx, # type: ignore\n",
    "            ))\n",
    "            \n",
    "            for index in range(len(data)):\n",
    "                if index < 50:  # type: ignore\n",
    "                    continue\n",
    "                if index >= 100:  # type: ignore\n",
    "                    break\n",
    "                print(f\"Dataset: {dataset['nome']} Modelo: {modelo} Instancia: {index}\")\n",
    "\n",
    "                # Predicao\n",
    "                network_input = tf.reshape(tf.constant(data.iloc[index, :-1]), (1, -1))  # type: ignore\n",
    "                network_output = tf.argmax(keras_model.predict(tf.constant(network_input), verbose=0)[0])  # type: ignore\n",
    "\n",
    "                # # Explicacao\n",
    "                # init = time()\n",
    "                # (explanation, _) = get_minimal_explanation(\n",
    "                #     model_milp.clone(),\n",
    "                #     network_input,\n",
    "                #     network_output,\n",
    "                #     n_classes=dataset[\"n_classes\"],\n",
    "                #     method=method,\n",
    "                #     output_bounds=output_bounds,\n",
    "                # )\n",
    "                # end = time()\n",
    "\n",
    "                # original = {\n",
    "                #     \"time_milp\": end - init,\n",
    "                #     \"len_milp\": len(explanation),\n",
    "                #     \"explanation\": linear_constraints_to_dict(explanation),\n",
    "                # }\n",
    "\n",
    "                # Relaxed\n",
    "                relaxed = {\"times\": [], \"len_eq\": [], \"len_range\": [], \"explanation\": []}\n",
    "                for delta in deltas:\n",
    "                    init = time()\n",
    "                    (explanation, _) = get_explanation_range(\n",
    "                        milp_model=model_milp_relaxed.clone(),\n",
    "                        network_input=network_input,\n",
    "                        n_classes=dataset[\"n_classes\"],\n",
    "                        delta=delta,\n",
    "                        network_output=network_output,\n",
    "                    )\n",
    "                    end = time()\n",
    "                    relaxed[\"times\"].append(end - init)\n",
    "                    relaxed[\"len_eq\"].append(len(explanation[\"eq\"]))\n",
    "                    relaxed[\"len_range\"].append(len(explanation[\"range\"]))\n",
    "                    relaxed[\"explanation\"].append({\n",
    "                        \"eq\": linear_constraints_to_dict(explanation[\"eq\"]),\n",
    "                        \"range\": explanation[\"range\"],\n",
    "                    })\n",
    "\n",
    "                store_data(\n",
    "                    f\"{results_path}/{dataset['nome']}.csv\",\n",
    "                    [\n",
    "                        {\n",
    "                            \"dataset\": dataset[\"nome\"],\n",
    "                            \"modelo\": modelo,\n",
    "                            \"instance\": index,\n",
    "                            # **{f\"original_{k}\": v for k, v in original.items()},\n",
    "                            **{f\"relaxed_{k}\": v for k, v in relaxed.items()},\n",
    "                        }\n",
    "                    ],\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 50\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 51\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 52\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 53\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 54\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 55\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 56\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 57\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 58\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 59\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 60\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 61\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 62\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 63\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 64\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 65\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 66\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 67\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 68\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 69\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 70\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 71\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 72\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 73\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 74\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 75\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 76\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 77\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 78\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 79\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 80\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 81\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 82\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 83\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 84\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 85\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 86\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 87\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 88\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 89\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 90\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 91\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 92\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 93\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 94\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 95\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 96\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 97\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 98\n",
      "Dataset: digits Modelo: model_3layers_20neurons.h5 Instancia: 99\n"
     ]
    }
   ],
   "source": [
    "layers = []\n",
    "path = os.path.join(\n",
    "    \"results\",\n",
    "    \"relaxed\",\n",
    "    \"get_explanation_range\",\n",
    "    # f\"layers {'_'.join([str(l) for l in layers])} relaxed\",\n",
    "    \"full fixed\",\n",
    ")\n",
    "\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "benchmark(path, layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
